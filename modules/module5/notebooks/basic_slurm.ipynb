{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae98056",
   "metadata": {},
   "source": [
    "# Module 5 - Read QC with a helpful hand from slurm\n",
    "\n",
    "In this notebook, you will learn the basics of using SLURM (Simple Linux Utility for Resource Management) to submit jobs to the Rockfish cluster.\n",
    "\n",
    "**This notebook must be run on the Rockfish cluster (edulogin.arch.jhu.edu) using the login credentials provided on Monday.**\n",
    "\n",
    "**Running this notebook on your local machine is not be possible.**\n",
    "\n",
    "We will use SLURM to run the read quality control program `fastqc` on a collection of fastq files that we provide.\n",
    "\n",
    "## Learning Objectives\n",
    "- Familiarize yourself with the basic SLURM commands.\n",
    "- Learn how to submit jobs to the cluster using the SLURM command `sbatch`.\n",
    "- Learn how to monitor jobs using the SLURM command `sacct`.\n",
    "- Understand where and how to find the output of your jobs.\n",
    "- Gain experience in reviewing the output of fastqc to assess the quality of the reads in a fastq file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc6d9b",
   "metadata": {},
   "source": [
    "## The Task\n",
    "We have provided you with a set of raw read fastq files in the directory `/data/me440_lgoff2/datasets/` on the Rockfish cluster.\n",
    "Your task is to copy the fastq files to your working directory, run `fastqc` on each of these files, and review the output to assess the quality of the reads in each file.\n",
    "\n",
    "You will submit these jobs to the cluster using SLURM.\n",
    "\n",
    "## The Tools\n",
    "We will be using the following tools:\n",
    "- `sbatch` - submit a job to the cluster (SLURM is already provided/installed on the cluster so we do not need to add anything to our environment)\n",
    "- `fastqc` - a program that assesses the quality of reads in a fastq file\n",
    "- `multiqc` - a program that aggregates the output of multiple steps in a bioinformatics workflow (including fastqc runs) into a single report.\n",
    "\n",
    "To install `fastqc` and `multiqc` on your Rockfish account, we will use the following command:\n",
    "\n",
    "_You will only need to run this once to install._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mamba install -c bioconda fastqc multiqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c72e6",
   "metadata": {},
   "source": [
    "**You will only need to run this once to install.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7833a73",
   "metadata": {},
   "source": [
    "## The Data\n",
    "Next, we will copy the fastq files which are already stored on the cluster in a shared directory.\n",
    "\n",
    "Let's start by creating a new directory 'data' in our current working directory to store the fastq files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c84de84",
   "metadata": {},
   "source": [
    "Now, let's copy the fastq files from the shared directory to our local directory.\n",
    "\n",
    "These are the raw RNA-Seq reads for the HippoSeq dataset [GSE74985](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE74985).\n",
    "\n",
    "The samples for this study were each sequenced once (1 run per sample) on the Illumina sequencing platform to generate single-end reads of 100bp in length.\n",
    "\n",
    "The directory that contains the gzip-compressed .fastq.gz files is `/data/me440_lgoff2/datasets/RNA-Seq/data/raw/GSE74985`.\n",
    "\n",
    "We'll use the `cp` command to copy all of the .fastq.gz files from the shared directory to our new local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /data/me440_lgoff2/datasets/RNA-Seq/data/raw/GSE74985/*.fastq.gz data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f5775",
   "metadata": {},
   "source": [
    "Let's take a look at the files we just copied over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55279d93",
   "metadata": {
    "title": "[bash]"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac856f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls data/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c36f0",
   "metadata": {},
   "source": [
    "There should be 24 *.fastq.gz files in the `data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78a20c",
   "metadata": {},
   "source": [
    "For this exercise, we want to run the read quality control program `fastqc` on each of these files.\n",
    "\n",
    "`fastqc` is a program that parses the reads in a fastq file and outputs some summary statistics,\n",
    "metrics, and heuristics to tell us more about what is in the file, and the quality of the reads. It generates a report that can be used to identify systematic issues/errors that can be common in the library preparation or sequencing of the reads.\n",
    "\n",
    "Let's run fastqc to see if we can get an idea of the quality of the reads in one of the files.\n",
    "\n",
    "First, let's create a directory to store the output of `fastqc` for each of the files. We'll call this directory `fastqc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir fastqc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5dbde",
   "metadata": {},
   "source": [
    "To generate the fastqc reports we need to run the following command for each file:\n",
    "\n",
    "```\n",
    "fastqc -o fastqc_output <fastq_file>\n",
    "```\n",
    "\n",
    "The `-o` flag tells `fastqc` where to store the output files, in this case,the directory `fastqc_output`.\n",
    "\n",
    "_Remember, for most tools we can either use the `man` command or the `--help` flag to get more information about the tool and how to use it.\n",
    "`fastqc` is no exception here and if we want to find other arguments that are available we can run:\n",
    "\n",
    "```\n",
    "fastqc --help\n",
    "```\n",
    "\n",
    "Let's try the first file `SRR2916027.fastq.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc14eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fastqc -o fastqc_output data/SRR2916027.fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207013b8",
   "metadata": {},
   "source": [
    "This generates a report for the file `SRR2916027.fastq.gz` and stores it in the directory `fastqc_output`. This process takes approximately 3 minutes to run.\n",
    "\n",
    "Let's take a look at the .html report that was generated. To do so, we can either download the file and open in a browser, or preview the file in vscode. Let's try the latter.\n",
    "\n",
    "(Cmd+Shift+P) -> HTML: Open Preview (_You may have to install the `HTML Preview` extension for VSCode_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4be33",
   "metadata": {},
   "source": [
    "We still have a number of files for which to run this report.\n",
    "We could run each job one at a time, which would require constant monitoring.\n",
    "\n",
    "Alternatively, we could write a for loop to run each job in succession, but it would still run each job sequentially (~3min * 24 jobs = ~72 minutes).\n",
    "\n",
    "Instead, we can use SLURM to submit each job to the cluster and let SLURM manage the resources, scheduling, and execution of each job in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865f860",
   "metadata": {},
   "source": [
    "### SLURM: An Introduction\n",
    "\n",
    "[Slurm](https://slurm.schedmd.com/quickstart.html) (Simple Linux Utility for Resource Management) is an open-source job scheduler that allocates compute resources on clusters for queued user jobs.\n",
    "Slurm has become a standard for supercomputing environments, providing both resource management and job scheduling. Slurm is used on the [rockfish cluster](http://edulogin.arch.jhu.edu).\n",
    "\n",
    "Slurm is a command-line tool that can be used to submit, monitor, and cancel jobs.\n",
    "\n",
    "Slurm is _primarily_ useful when we need to run a large number of the same type of jobs that can be run independently of each other.\n",
    "This type of problem is called 'embarrasingly parallel'. Instead of running each of the jobs in succession, we can submit them all at once and let Slurm manage the resources, scheduling, and execution of each job in parallel.\n",
    "\n",
    "This is the case for many bioinformatics pipelines, including for example, the alignment of many samples to a reference genome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa97fc6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Job Submission with `sbatch`\n",
    "\n",
    "The `sbatch` command is used to submit a job for execution to the SLURM scheduler.\n",
    "A 'job' in this context is a set of commands you wish to execute.\n",
    "\n",
    "You typically provide `sbatch` with a script containing directives and commands.\n",
    "However, for simpler cases like ours, the `--wrap` argument allows direct submission of command-line calls.\n",
    "\n",
    "#### Submission using `--wrap`:\n",
    "\n",
    "Let's say we want to run the following command:\n",
    "\n",
    "`echo 'Hello World!'`\n",
    "\n",
    "We can submit this job using `sbatch` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b47798",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sbatch --wrap=\"echo 'Hello World!'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50318d94",
   "metadata": {},
   "source": [
    "Upon submission, SLURM returns a `job id` number. The output of the job is written to a file named `slurm-<job_id>.out` in the current working directory. This file captures the STDOUT, which is what would normally be printed to the terminal. In our case, it contains the string 'Hello World!'.\n",
    "\n",
    "Let's examine the contents of this 'output' file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6949e",
   "metadata": {},
   "source": [
    "### Applying `sbatch` to the `fastqc` example\n",
    "We want to run the following command for each file:\n",
    "\n",
    "`fastqc -o fastqc_output <fastq_file>`\n",
    "\n",
    "We can submit this job using `sbatch` as follows:\n",
    "\n",
    "`sbatch --wrap=\"fastqc -o fastqc_output <fastq_file>\"`\n",
    "\n",
    "Let's rerun the first file `SRR2916027.fastq.gz` using `sbatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f17f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sbatch --wrap=\"fastqc -o fastqc_output data/SRR2916027.fastq.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03067678",
   "metadata": {},
   "source": [
    "The job is now 'submitted' to the SLURM scheduler and we have a new `job_id` number.\n",
    "We can check the corresponding `.out` file to see the output (STDOUT) that would have been printed to the screen.\n",
    "\n",
    "Note that this job execution doesn't tie up our terminal resources as it runs elsewhere on the cluster,\n",
    "allowing us to continue working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbcd3f",
   "metadata": {},
   "source": [
    "To check the status of our jobs\n",
    "we use the SLURM command `sacct` ([SLURM 'Accounting'](https://slurm.schedmd.com/sacct.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sacct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f2c58",
   "metadata": {},
   "source": [
    "We can see that our job is currently running.\n",
    "The output includes the job id number, the user who submitted the job, the start time, the partition,\n",
    "the state, and any exit codes (errors) that the job may have produced (normal exit is 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7a608",
   "metadata": {},
   "source": [
    "To streamline the submission and execution of the `fastqc` jobs for the `.fastq.gz` files in our `data` directory,\n",
    "we can use a bash `for` loop to iterate over each file and submit a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in data/*.fastq.gz\n",
    "do\n",
    "\tsbatch --wrap=\"fastqc -o fastqc_output $file\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ade515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "for file in Path(\"data\").iterdir():\n",
    "    if file.suffix == \".fastq.gz\":\n",
    "        subprocess.run(\n",
    "            f'sbatch --wrap=\"fastqc -o fastqc_output {file}\"',\n",
    "            check=True,\n",
    "            shell=True\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3ea69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's check the status of our jobs again using `sacct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sacct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425fd1e",
   "metadata": {},
   "source": [
    "Now we can see more jobs in the queue.\n",
    "Some are running, some might have completed, and some might still be 'PENDING'.\n",
    "SLURM balances the available resources on the cluster to ensure all jobs complete in a timely manner,\n",
    "queuing some jobs until resources become available.\n",
    "\n",
    "SLURM generally follows a 'first-come, first-served' model,\n",
    "where jobs submitted first have higher priority for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311ef0f",
   "metadata": {},
   "source": [
    "### Monitoring All Jobs with `squeue`\n",
    "\n",
    "While `sacct` is useful for monitoring your own jobs,\n",
    "`squeue` displays the status of all 'active' jobs on the cluster, including those submitted by other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91430447",
   "metadata": {
    "title": "[bash]"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212cc61",
   "metadata": {},
   "source": [
    "The output shows a list of jobs in the queue,\n",
    "distributed across different compute nodes (NODELIST) on the cluster.\n",
    "This gives an idea of the cluster load and available resources.\n",
    "To focus on _your_ specific jobs, use the `-u` flag to filter by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "squeue -u lgoff2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418c785",
   "metadata": {},
   "source": [
    "### Cancelling Jobs with `scancel`\n",
    "\n",
    "`scancel` is used to terminate 'RUNNING' or 'PENDING' jobs.\n",
    "\n",
    "The `scancel` command is used to terminate 'RUNNING' or 'PENDING' jobs.\n",
    "\n",
    "#### Cancel a Specific Job:\n",
    "To cancel a specific job, provide the job id number to the `scancel` command:\n",
    "\n",
    "```\n",
    "scancel <job_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020789d9",
   "metadata": {},
   "source": [
    "#### Cancel All Jobs for a User:\n",
    "If you've made an error while submitting a large number of jobs, you can cancel all jobs for a specific user:\n",
    "\n",
    "```\n",
    "scancel -u lgoff2\n",
    "```\n",
    "\n",
    "However, caution is advised as this will cancel **all** of your jobs, including those that are maintaining your ssh tunnel to allow VSCode to connect. Doing this will disconnect you from the cluster and you will have to reconnect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de666aa",
   "metadata": {},
   "source": [
    "### Inspecting Cluster with `sinfo`\n",
    "As we delve deeper into using the cluster and SLURM,\n",
    "it may be useful to understand more about the cluster's configuration and the resources available for SLURM.\n",
    "\n",
    "The `sinfo` command provides an overview of SLURM nodes (compute nodes) and partitions ('queues' for job submission)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15277477",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177e2dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The output tells us that there is one (unique) partition or 'queue' for job submission called `defq` (for 'default queue').\n",
    "It is currently available (up) with a 2-hour time limit per job (`2:00:00`).\n",
    "\n",
    "It also informs us about the number of compute nodes available for this partition (6)\n",
    "and the 'NODELIST' provides the 'names' of the compute nodes included in this partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9b13e",
   "metadata": {},
   "source": [
    "##### Node-specific Information\n",
    "We can also get more information about the compute nodes using the `-N` flag\n",
    "\n",
    "`sinfo -N`\n",
    "\n",
    "Let's also add the `-l` flag to give us more information in a 'long' format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae54483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sinfo -N -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f66d62",
   "metadata": {},
   "source": [
    "Here we can see that each of the compute nodes has 24 available CPUs and 91552 MB (~92 Gb) of memory available for jobs.\n",
    "\n",
    "These are the resources that slurm is managing and allocating to jobs that are submitted to the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4b70b",
   "metadata": {},
   "source": [
    "### Running Jobs Interactively with SLURM\n",
    "\n",
    "We've already seen `srun` in action when we used it to create an interactive session and the SSH tunnel on the cluster. Let's revisit this command to understand its components better.\n",
    "\n",
    "```bash\n",
    "ssh lgoff2@edulogin.arch.jhu.edu \"srun --time=2:00:00 --mem-per-cpu=4GB --cpus-per-task=2 VSCode-linux-x64/bin/code tunnel --accept-server-license-terms\"\n",
    "````\n",
    "\n",
    "The above command was used to ssh into the cluster using my credentials (lgoff2@edulogin.arch.jhu.edu), and then immediately execute a call to srun on the login node.\n",
    "\n",
    "Here, srun initiates an interactive session on the cluster with the following resources:\n",
    "\n",
    "- --time=2:00:00 - 2 hours of walltime\n",
    "- --mem-per-cpu=4GB - 4 GB of memory per CPU\n",
    "- --cpus-per-task=2 - 2 CPUs per task\n",
    "\n",
    "By default, `srun`` starts an interactive session and runs the provided commands.\n",
    "In this instance, we're asking srun to create an interactive session and run the following command immediately:\n",
    "\n",
    "```\n",
    "VSCode-linux-x64/bin/code tunnel --accept-server-license-terms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e306773",
   "metadata": {},
   "source": [
    "This command starts the VSCode server on the cluster and creates the SSH tunnel, enabling us to connect to the server.\n",
    "\n",
    "Interactive sessions on the cluster can be helpful when debugging or developing job submissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9df0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## MultiQC\n",
    "Having successfully executed fastq on each of our fastq files and generated a report for each file,\n",
    "we'd like to examine these reports to identify any unusual patterns or suspect read quality in these samples.\n",
    "\n",
    "However, parsing through 24 .html files to find patterns/trends can be tedious.\n",
    "This is where MultiQC comes in handy. [MultiQC](https://multiqc.info/) is a powerful tool for aggregating the output of multiple steps or samples in a bioinformatics workflow (including fastqc runs) into a single report.\n",
    "\n",
    "MultiQC traverses a directory, searches for output from common bioinformatics tools, extracts the data, and generates a single report summarizing the data.\n",
    "Let's use MultiQC to aggregate the output of our fastqc runs into a single report, which will simplify comparisons across our samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ca931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "multiqc ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530a523",
   "metadata": {},
   "source": [
    "Let's take a look at the summary report that was generated.\n",
    "\n",
    "See anything useful or interesting about the samples?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
