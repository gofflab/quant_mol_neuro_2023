{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Module 5 - Read QC with a helpful hand from slurm\n",
    "\n",
    " This notebook will introduce you to the basics of using slurm to submit jobs to the cluster.\n",
    "\n",
    " __This notebook must be run on the Rockfish (edulogin.arch.jhu.edu) cluster using the login credentials we provided on monday. It will not complete on your local machine.__\n",
    "\n",
    " We will use slurm to run the read qc program `fastqc` on a collection of fastq files provided for you.\n",
    "\n",
    " ## Learning Objectives\n",
    " - Familiarize yourself with the basic slurm commands\n",
    " - Learn how to submit jobs to the cluster using the slurm command `sbatch`\n",
    " - Learn how to monitor jobs using the slurm command `sacct`\n",
    " - Understand where and how to find the output of your jobs.\n",
    " - Gain experience in reviewing the output of fastqc to assess the quality of the reads in a fastq file.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The Task\n",
    " We have provided you with a set of raw read fastq files in the directory `/data/me440_lgoff2/datasets/` on the rockfish cluster.  Your task is to copy the fastq files to your working directory and run `fastqc` on each of these files and review the output to assess the quality of the reads in each file.\n",
    "\n",
    " You will submit these jobs to the cluster using slurm.\n",
    "\n",
    " ## The Tools\n",
    " We will be using the following tools:\n",
    " - `sbatch` - submit a job to the cluster (slurm is already provided/installed on the cluster so we do not need to add anything to our environment)\n",
    " - `fastqc` - a program that assesses the quality of reads in a fastq file\n",
    " - `multiqc` - a program that aggregates the output of multiple steps in a bioinformatics workflow (including fastqc runs) into a single report.\n",
    "\n",
    " To install `fastqc` and `multiqc` on your rockfish account, we will use the following command:\n",
    "\n",
    " _You will only need to run this once to install._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mamba install -c bioconda fastqc multiqc\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _You will only need to run this once to install._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## The Data\n",
    " Next, Let's make a copy of the fastq files which are already stored on the cluster in a shared directory.\n",
    "\n",
    " We'll start by creating a new directory 'data' in our current working directory to store the fastq files.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir data\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we'll copy the fastq files from the shared directory to our local directory.\n",
    "\n",
    " These are the raw RNA-Seq reads for the HippoSeq dataset [GSE74985](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE74985).\n",
    "\n",
    " The samples for this study were each sequenced once (1 run per sample) on the Illumina sequencing platform to generate single-end reads of 100bp in length.\n",
    "\n",
    " The directory that contains the gzip-compressed .fastq.gz files is `/data/me440_lgoff2/datasets/RNA-Seq/data/raw/GSE74985`.\n",
    "\n",
    " Let's use the `cp` command to copy all of the .fastq.gz files from the shared directory to our new local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /data/me440_lgoff2/datasets/RNA-Seq/data/raw/GSE74985/*.fastq.gz data/\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's take a look at the files we just copied over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls data/\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls data/ | wc -l\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There should be 24 *.fastq.gz files in the `data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this exercise we want to run the read quality control program `fastqc` on each of these files.\n",
    "\n",
    " `fastqc` is a program that parses the reads in a fastq file and outputs some summary statistics, metrics, and heuristics to tell us more about what is in the file, and the quality of the reads.  It generates a report that can be used to identify systematic issues/errors that can be common in the library preparation or sequencing of the reads.\n",
    "\n",
    " Let's run fastqc to see if we can get an idea of the quality of the reads in one of the files.\n",
    "\n",
    " First let's create a directory to store the output of `fastqc` for each of the files.  We'll call this directory `fastqc_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir fastqc_output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To generate the fastqc reports we need to run the following command for each file:\n",
    "\n",
    " `fastqc -o fastqc_output <fastq_file>`\n",
    "\n",
    " The `-o` flag tells `fastqc` where to store the output files, in this case, the directory `fastqc_output`.\n",
    "\n",
    " _Remember, for most tools we can either use the `man` command or the `--help` flag to get more information about the tool and how to use it. `fastqc` is no exception here and if we want to find other arguments that are available we can run:_\n",
    "\n",
    " `fastqc --help`\n",
    " Let's try the first file `SRR2916027.fastq.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fastqc -o fastqc_output data/SRR2916027.fastq.gz\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This generates a report for the file `SRR2916027.fastq.gz` and stores it in the directory `fastqc_output` and takes ~3 minutes to run.\n",
    "\n",
    " Let's take a look at the .html report that was generated. To do so, we can either download the file and open in a browser, or preview the file in vscode.  Let's try the latter.\n",
    "\n",
    " (Cmd+Shift+P) -> HTML: Open Preview (_You may have to install the `HTML Preview` extension for VSCode_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We still have a number of files for which to run this report.  We could run each job one at a time, but that would take a lot of babysitting.\n",
    "\n",
    " We could _also_ write a for loop to run each job in succession but it would still run each job sequentially (~3min * 24 jobs = ~72 minutes).\n",
    "\n",
    " Instead, we can use slurm to submit each job to the cluster and let slurm manage the resources, scheduling, and execution of each job in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM: An Introduction\n",
    "\n",
    " [Slurm](https://slurm.schedmd.com/quickstart.html) (Simple Linux Utility for Resource Management) is an open-source job scheduler that allocates compute resources on clusters for queued user jobs.\n",
    " Slurm has become a standard for supercomputing environments, providing both resource management and job scheduling. Slurm is used on the [rockfish cluster](http://edulogin.arch.jhu.edu).\n",
    "\n",
    " Slurm is a command-line tool that can be used to submit, monitor, and cancel jobs.\n",
    "\n",
    " Slurm is _primarily_ useful when we need to run a large number of the same type of jobs that can be run independently of each other. This type of problem is called 'embarrasingly parallel'. Instead of running each of the jobs in succession, we can submit them all at once and let Slurm manage the resources, scheduling, and execution of each job in parallel.\n",
    "\n",
    " This is the case for many bioinformatics pipelines, including for example, the alignment of many samples to a reference genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Jobs with `sbatch`\n",
    "\n",
    " `sbatch` is used to submit a job to the scheduler for execution.\n",
    " A 'job' is a set of commands that you would like to execute.\n",
    "\n",
    " Typically, you provide `sbatch` with a script that provides directives and commands, but for simpler use cases like this, the `--wrap` argument allows for direct submission of command-line calls.\n",
    "\n",
    " #### Submission using `--wrap`:\n",
    "\n",
    " let's say we wanted to run the following command:\n",
    "\n",
    " `echo 'Hello World!'`\n",
    "\n",
    " We could submit this job using `sbatch` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sbatch --wrap=\"echo 'Hello World!'\"\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice the only information that we get from this is an acknowledgement that the job was submitted and a `job id` number.\n",
    "\n",
    " By default, the output of the job is written to a file called `slurm-<job_id>.out` in the current working directory. This file captures the STDOUT produced by the job (what is normally printed to the terminal.)\n",
    "\n",
    " Because the job was submitted and executed on the cluster, the output file is written to this file so we have a record of what _would_ have been printed to the screen, in this case, the string 'Hello World!'.\n",
    "\n",
    " Let's take a look at the contents of this 'output' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Back to the matter at hand\n",
    " Let's revisit our fastqc example.  We want to run the following command for each file:\n",
    "\n",
    " `fastqc -o fastqc_output <fastq_file>`\n",
    "\n",
    " We can submit this job using `sbatch` as follows:\n",
    "\n",
    " `sbatch --wrap=\"fastqc -o fastqc_output <fastq_file>\"`\n",
    "\n",
    " Let's rerun the first file `SRR2916027.fastq.gz` using `sbatch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sbatch --wrap=\"fastqc -o fastqc_output data/SRR2916027.fastq.gz\"\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The job has now been 'submitted' to the slurm scheduler and we have a new `job_id` number. We can open the corresponding `.out` file to watch as the output that was previously printed to the screen is written to the file.\n",
    "\n",
    " It's also important to note that this no longer ties up our available resources in the terminal to run this job.  It's actually being executed somewhere else on the cluster.  So we can keep working while the job is being executed.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can check the status of our jobs using the slurm command `sacct` ([Slurm 'Accounting'](https://slurm.schedmd.com/sacct.html)).  Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sacct\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that our job is currently running.\n",
    "\n",
    " We can also see the job id number, the user who submitted the job, the start time, the partition, the state, and any exit codes (errors) that might have been produced by the job.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ok, let's try and save ourselves some time and parallelize the submition and execution of each of the fastqc jobs for the `.fastq.gz` files in our `data` directory.\n",
    "\n",
    " We could write out each of the jobs that we want to execute and wrap each in an `sbatch` call, but that could be tedious and error prone.\n",
    "\n",
    " Instead, we can use a bash `for` loop to iterate over each of the files in our `data` directory and submit a job for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in data/*.fastq.gz\n",
    "do\n",
    "\tsbatch --wrap=\"fastqc -o fastqc_output $file\"\n",
    "done\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's check the status of our jobs again using `sacct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sacct\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we should see a lot more jobs in the queue. We can see that some of the jobs are running, some may have completed, and some may still be 'PENDING'.\n",
    "\n",
    " This is because slurm is 'balancing' the available resources on the cluster to ensure that all jobs are able to complete in a timely manner.  This means that some jobs may be queued until there are enough resources available to execute them.\n",
    "\n",
    " In general, slurm is a 'first-come, first-served' model in which jobs that are submitted first will have more priority for execution than those submitted later.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Monitoring All Jobs with `squeue`\n",
    "\n",
    " While `sacct` can be useful for monitoring your own jobs, `squeue` displays the status of all 'active' jobs on the cluster, including information about those submitted from other users.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "squeue\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that there are a lot of jobs in the queue.  We can also see that the jobs are being distributed across the different compute nodes (NODELIST) on the cluster.\n",
    "\n",
    " This can give you a feel for how busy the cluster is and how many resources might be available for your jobs.\n",
    "\n",
    " To focus again on _your_ specific jobs, we can use the `-u` flag to filter by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "squeue -u lgoff2\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancelling Jobs with `scancel`\n",
    "\n",
    "`scancel` is used to terminate 'RUNNING' or 'PENDING' jobs.\n",
    "\n",
    "#### Cancel a Specific Job:\n",
    " You can cancel a specific job by providing the job id number to the `scancel` command:\n",
    "\n",
    " `scancel <job_id>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancel All Jobs for a User:\n",
    " Additionally, if you found you made a mistake while submitting a large number of jobs, you can cancel all jobs for a specific user:\n",
    "\n",
    " `scancel -u lgoff2`\n",
    "\n",
    " _A word of caution however, this will cancel **all** of your jobs, including the job that was submitted when we created our ssh tunnel to allow VSCode to connect.  Doing this will boot you off of the cluster and you will have to reconnect._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Cluster with `sinfo`\n",
    " As we learn more about how to use the cluster and slurm, it may be useful to learn more about how the cluster is configured and what resources are available for slurm.\n",
    "\n",
    " `sinfo` provides an overview of Slurm nodes (compute nodes) and partitions ('queues' for job submission).\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sinfo\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This tells us that there is one (unique) partition or 'queue' for job submission called `defq` (for 'default queue'). It is currently available (up) with a 2 HR timelimit per job (`2:00:00`).\n",
    "\n",
    " It also tells us the number of compute nodes available for this partition (6) and the 'NODELIST' gives us the 'names' of the compute nodes that are included as part of this partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Node-specific Information\n",
    " We can also get more information about the compute nodes using the `-N` flag\n",
    "\n",
    " `sinfo -N`\n",
    "\n",
    " Let's also add the `-l` flag to give us more information in a 'long' format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sinfo -N -l\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we can see that each of the compute nodes has 24 available CPUs and 91552 MB (~92 Gb) of memory available for jobs.\n",
    "\n",
    " These are the resources that slurm is managing and allocating to jobs that are submitted to the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Jobs Interactively with `srun`\n",
    " We've already seen `srun` in action when we used it to create an interactive session and the ssh tunnel on the cluster.\n",
    "\n",
    " Let's revisit this command again to deconstruct what it is doing.\n",
    "\n",
    " `ssh lgoff2@edulogin.arch.jhu.edu \"srun --time=2:00:00 --mem-per-cpu=4GB --cpus-per-task=2 VSCode-linux-x64/bin/code tunnel --accept-server-license-terms\"`\n",
    "\n",
    " The above command was used to `ssh` into the cluster with my credentials (lgoff2@edulogin.arch.jhu.edu), and then immediately excute a call to `srun` on the login node.\n",
    "\n",
    " `srun` was used to initiate an interactive session on the cluster with the following resources:\n",
    " - `--time=2:00:00` - 2 hours of walltime\n",
    " - `--mem-per-cpu=4GB` - 4 Gb of memory per cpu\n",
    " - `--cpus-per-task=2` - 2 cpus per task\n",
    "\n",
    " By default `srun` will start an interactive session and run whatever commands are provided.  He're asking `srun` to create an interactive session and _immediately_ run the following command in the interactive session:\n",
    "\n",
    " `VSCode-linux-x64/bin/code tunnel --accept-server-license-terms`\n",
    "\n",
    " This will start the VSCode server on the cluster and create the ssh tunnel which allows us to connect to the server.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Often times when we're debugging and or fleshing out job submissions, having an interactive session on the cluster is helpful.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## MultiQC\n",
    " Ok, back to the matter at hand, we have successfully executed `fastq` on each of our fastq files and generated a report for each file.\n",
    "\n",
    " We'd like to actually start to look through each of these reports to see if anything looks odd, or otherwise suspect in terms of read quality for these samples.\n",
    "\n",
    " But parsing through 24 .html files to find patterns/trends might be a bit tedious.\n",
    "\n",
    " [MultiQC](https://multiqc.info/) is an exceptionally useful tool for aggregating the output of multiple steps or samples in a bioinformatics workflow (including fastqc runs) into a single report.\n",
    "\n",
    " MultiQC traverses a directory and searches for output from common bioinformatics tools, extracts the data, and generates a single report summarizing the data.\n",
    "\n",
    " Let's use MultiQC to aggregate the output of our fastqc runs into a single report that will be much easier to make comparisons across our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "multiqc .\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's take a look at the summary report that was generated.\n",
    "\n",
    " See anything useful or interesting about the samples?"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}